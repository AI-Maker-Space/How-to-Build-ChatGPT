{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI Agents SDK - Calendar Research Assistant\n",
        "\n",
        "Welcome to AI Makerspace! This notebook demonstrates the powerful new **OpenAI Agents SDK**, which provides a framework for building intelligent agents that can interact with external tools and services.\n",
        "\n",
        "## What We'll Build\n",
        "\n",
        "A **Calendar Research Assistant** that:\n",
        "1. **Connects to your Google Calendar** to fetch events for a specific day\n",
        "2. **Analyzes each event** to understand what preparation might be needed\n",
        "3. **Performs web research** to gather relevant information about event topics\n",
        "4. **Creates structured preparation guides** with key insights and recommendations\n",
        "\n",
        "## Key Features of the Agents SDK\n",
        "\n",
        "- **Tool Integration** - Connect to external services like calendars and web search\n",
        "- **Structured Reasoning** - Built-in reasoning controls for complex decision-making\n",
        "- **Async Support** - Efficient parallel processing of multiple tasks\n",
        "- **Streaming Responses** - Real-time updates as the agent works\n",
        "- **Type Safety** - Pydantic models for structured inputs and outputs\n",
        "- **MCP Connectors** - Model Context Protocol support for service integration\n",
        "\n",
        "This notebook walks through building a practical agent that helps you prepare for meetings and events by automatically researching relevant topics and providing actionable insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup and Authentication\n",
        "\n",
        "First, we need to set up our authentication. We'll need:\n",
        "1. **OpenAI API Key** - For the agents and language models\n",
        "2. **Google Calendar Authorization** - To access your calendar events\n",
        "\n",
        "For Google Calendar:\n",
        "1. Visit [Google OAuth Playground](https://developers.google.com/oauthplayground/)\n",
        "2. Input `https://www.googleapis.com/auth/calendar.events` as the required scope\n",
        "3. Click \"Authorize APIs\" and go through the OAuth flow\n",
        "4. Click \"Exchange authorization code for tokens\"\n",
        "5. Copy the access token (starts with \"ya29.\")\n",
        "\n",
        "> **Note**: The access token expires after about an hour. For production use, implement proper OAuth refresh token handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Follow the instructions above to get your Google Calendar access token.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Set up OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# Set up Google Calendar Authorization\n",
        "print(\"\\nFollow the instructions above to get your Google Calendar access token.\")\n",
        "os.environ[\"GOOGLE_CALENDAR_AUTHORIZATION\"] = getpass.getpass(\"Google Calendar Access Token (ya29...): \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Required Libraries\n",
        "\n",
        "Now we'll import the necessary components from the OpenAI Agents SDK and other required libraries. The SDK provides:\n",
        "\n",
        "- `Agent` - The core agent class that defines behavior and capabilities\n",
        "- `HostedMCPTool` - Model Context Protocol tool for connecting to external services\n",
        "- `WebSearchTool` - Built-in tool for web search capabilities\n",
        "- `Runner` - Executes agents with their inputs\n",
        "- `ModelSettings` - Configuration for model behavior and reasoning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from agents import Agent, HostedMCPTool, WebSearchTool, Runner, ModelSettings\n",
        "from openai.types.shared.reasoning import Reasoning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Structured Output Models\n",
        "\n",
        "We'll use Pydantic models to define structured outputs for our agents. This ensures type safety and provides clear contracts for the data our agents will produce.\n",
        "\n",
        "These models define:\n",
        "- **CalendarEvent** - Structure for calendar event data\n",
        "- **EventResearch** - Research findings about a specific event\n",
        "- **PreparationGuide** - Actionable preparation recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CalendarEvent(BaseModel):\n",
        "    \"\"\"Represents a calendar event with essential details.\"\"\"\n",
        "    title: str\n",
        "    start_time: str\n",
        "    end_time: str\n",
        "    location: Optional[str]\n",
        "    attendees: List[str]\n",
        "    description: Optional[str]\n",
        "    \n",
        "class ResearchTopic(BaseModel):\n",
        "    \"\"\"A specific topic to research related to an event.\"\"\"\n",
        "    topic: str\n",
        "    relevance: str\n",
        "    search_query: str\n",
        "\n",
        "class EventResearch(BaseModel):\n",
        "    \"\"\"Research findings for a calendar event.\"\"\"\n",
        "    event_title: str\n",
        "    event_type: str  # meeting, presentation, interview, etc.\n",
        "    key_topics: List[str]\n",
        "    research_queries: List[ResearchTopic]\n",
        "    \n",
        "class PreparationItem(BaseModel):\n",
        "    \"\"\"A specific preparation action item.\"\"\"\n",
        "    action: str\n",
        "    priority: str  # high, medium, low\n",
        "    time_estimate: str\n",
        "    resources: List[str]\n",
        "\n",
        "class PreparationGuide(BaseModel):\n",
        "    \"\"\"Complete preparation guide for an event.\"\"\"\n",
        "    event_title: str\n",
        "    event_time: str\n",
        "    summary: str\n",
        "    key_insights: List[str]\n",
        "    preparation_checklist: List[PreparationItem]\n",
        "    talking_points: List[str]\n",
        "    questions_to_ask: List[str]\n",
        "    potential_challenges: List[str]\n",
        "    recommended_reading: List[str]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Calendar Agent\n",
        "\n",
        "Our first agent connects to Google Calendar using the MCP (Model Context Protocol) connector. This agent:\n",
        "\n",
        "- Uses `HostedMCPTool` to integrate with Google Calendar\n",
        "- Fetches events for a specific date\n",
        "- Returns structured event information\n",
        "- Handles authorization through the OAuth token we provided\n",
        "\n",
        "The MCP connector provides a standardized way to interact with external services, handling the complexity of API calls behind the scenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calendar_agent = Agent(\n",
        "    name=\"CalendarAssistant\",\n",
        "    instructions=\"\"\"You are a calendar assistant that helps users fetch and understand their schedule.\n",
        "    When asked about events for a specific day, retrieve all calendar events for that day and provide\n",
        "    a clear, structured summary of the schedule. Include event titles, times, locations, and attendees.\"\"\",\n",
        "    tools=[\n",
        "        HostedMCPTool(\n",
        "            tool_config={\n",
        "                \"type\": \"mcp\",\n",
        "                \"server_label\": \"google_calendar\",\n",
        "                \"connector_id\": \"connector_googlecalendar\",\n",
        "                \"authorization\": os.environ[\"GOOGLE_CALENDAR_AUTHORIZATION\"],\n",
        "                \"require_approval\": \"never\",\n",
        "            }\n",
        "        )\n",
        "    ],\n",
        "    model=\"gpt-5\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Event Analyzer Agent\n",
        "\n",
        "The Event Analyzer examines calendar events and determines what research topics would be most helpful for preparation. It:\n",
        "\n",
        "- Analyzes event titles, descriptions, and attendee lists\n",
        "- Identifies the type of event (meeting, presentation, interview, etc.)\n",
        "- Generates relevant research queries\n",
        "- Uses structured output for consistent results\n",
        "- Applies medium reasoning effort for thoughtful analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "event_analyzer_agent = Agent(\n",
        "    name=\"EventAnalyzer\",\n",
        "    instructions=\"\"\"You are an expert at analyzing calendar events to determine what preparation would be helpful.\n",
        "    Given a calendar event, analyze its title, description, attendees, and context to:\n",
        "    1. Identify the type of event (meeting, presentation, interview, workshop, etc.)\n",
        "    2. Extract key topics that should be researched\n",
        "    3. Generate 3-5 specific, actionable research queries that would help someone prepare\n",
        "    \n",
        "    Focus on practical, relevant research that would actually help in the meeting.\n",
        "    For example:\n",
        "    - For a product meeting: latest features, competitor analysis, user feedback\n",
        "    - For a technical discussion: relevant technologies, best practices, recent developments\n",
        "    - For a client meeting: company background, recent news, industry trends\n",
        "    \n",
        "    Make search queries specific and likely to return useful results.\"\"\",\n",
        "    model=\"gpt-5\",\n",
        "    model_settings=ModelSettings(reasoning=Reasoning(effort=\"medium\")),\n",
        "    output_type=EventResearch,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Research Agent\n",
        "\n",
        "The Research Agent performs web searches to gather information about event topics. This agent:\n",
        "\n",
        "- Uses the built-in `WebSearchTool` for real-time web searches\n",
        "- Processes multiple search queries in parallel for efficiency\n",
        "- Summarizes findings into digestible insights\n",
        "- Focuses on recent, relevant information\n",
        "- Uses `tool_choice=\"required\"` to ensure it always performs searches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "research_agent = Agent(\n",
        "    name=\"ResearchAgent\",\n",
        "    instructions=\"\"\"You are a research assistant specializing in gathering relevant information for meeting preparation.\n",
        "    Given a search query, perform a comprehensive web search and provide a concise summary of the findings.\n",
        "    \n",
        "    Focus on:\n",
        "    - Recent information (prioritize content from the last 6 months)\n",
        "    - Authoritative sources\n",
        "    - Practical insights that would be useful in a meeting context\n",
        "    - Key facts, trends, and developments\n",
        "    \n",
        "    Your summary should be 2-3 paragraphs, highlighting the most important and actionable information.\n",
        "    Always cite sources when possible and note any particularly interesting or surprising findings.\"\"\",\n",
        "    model=\"gpt-4.1\",\n",
        "    tools=[WebSearchTool()],\n",
        "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Preparation Guide Agent\n",
        "\n",
        "The Preparation Guide Agent synthesizes all research into an actionable preparation guide. Features:\n",
        "\n",
        "- Combines event details with research findings\n",
        "- Generates structured preparation checklists\n",
        "- Creates talking points and questions to ask\n",
        "- Identifies potential challenges\n",
        "- Provides recommended reading\n",
        "- Uses high reasoning effort for comprehensive analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "preparation_guide_agent = Agent(\n",
        "    name=\"PreparationGuideAgent\",\n",
        "    instructions=\"\"\"You are an expert meeting preparation coach who creates comprehensive, actionable preparation guides.\n",
        "    Given event details and research findings, create a detailed preparation guide that helps someone be fully prepared.\n",
        "    \n",
        "    Your guide should be:\n",
        "    - Practical and actionable with specific steps\n",
        "    - Prioritized by importance and urgency\n",
        "    - Time-conscious (realistic time estimates)\n",
        "    - Comprehensive but not overwhelming\n",
        "    \n",
        "    Include:\n",
        "    - A concise summary of what the meeting is about\n",
        "    - Key insights from the research\n",
        "    - Specific preparation actions with time estimates\n",
        "    - Smart talking points based on the research\n",
        "    - Thoughtful questions to ask during the meeting\n",
        "    - Potential challenges or sensitive topics to be aware of\n",
        "    - Links or resources for deeper reading\n",
        "    \n",
        "    Tailor the guide to the specific type of event (presentation, interview, client meeting, etc.)\"\"\",\n",
        "    model=\"gpt-5\",\n",
        "    model_settings=ModelSettings(reasoning=Reasoning(effort=\"high\")),\n",
        "    output_type=PreparationGuide,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the Calendar Research Orchestrator\n",
        "\n",
        "Now we'll create the main orchestrator function that coordinates all our agents. This function:\n",
        "\n",
        "1. **Fetches calendar events** for the specified date\n",
        "2. **Analyzes each event** to determine research needs  \n",
        "3. **Performs parallel research** on all identified topics\n",
        "4. **Generates preparation guides** for each event\n",
        "5. **Returns comprehensive results** with all preparation materials\n",
        "\n",
        "The orchestrator uses `asyncio` for efficient parallel processing, allowing multiple research queries to run simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CalendarResearchOrchestrator:\n",
        "    \"\"\"Orchestrates the calendar research workflow.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.calendar_agent = calendar_agent\n",
        "        self.analyzer_agent = event_analyzer_agent\n",
        "        self.research_agent = research_agent\n",
        "        self.guide_agent = preparation_guide_agent\n",
        "        \n",
        "    async def research_event(self, event_info: str) -> dict:\n",
        "        \"\"\"Research a single calendar event.\"\"\"\n",
        "        print(f\"\\n📅 Analyzing event...\")\n",
        "        \n",
        "        # Analyze the event to determine research topics\n",
        "        analysis_result = await Runner.run(\n",
        "            self.analyzer_agent,\n",
        "            f\"Analyze this calendar event and determine what research would be helpful:\\n\\n{event_info}\"\n",
        "        )\n",
        "        event_research = analysis_result.final_output_as(EventResearch)\n",
        "        \n",
        "        # Perform research on all topics in parallel\n",
        "        print(f\"🔍 Researching {len(event_research.research_queries)} topics...\")\n",
        "        research_tasks = []\n",
        "        for topic in event_research.research_queries:\n",
        "            task = asyncio.create_task(self.perform_research(topic.search_query))\n",
        "            research_tasks.append(task)\n",
        "        \n",
        "        research_results = await asyncio.gather(*research_tasks)\n",
        "        \n",
        "        # Combine all research into a preparation guide\n",
        "        print(\"📝 Creating preparation guide...\")\n",
        "        research_summary = \"\\n\\n\".join([r for r in research_results if r])\n",
        "        \n",
        "        guide_input = f\"\"\"\n",
        "        Event Details:\n",
        "        {event_info}\n",
        "        \n",
        "        Event Analysis:\n",
        "        Type: {event_research.event_type}\n",
        "        Key Topics: {', '.join(event_research.key_topics)}\n",
        "        \n",
        "        Research Findings:\n",
        "        {research_summary}\n",
        "        \"\"\"\n",
        "        \n",
        "        guide_result = await Runner.run(\n",
        "            self.guide_agent,\n",
        "            guide_input\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"event\": event_info,\n",
        "            \"analysis\": event_research,\n",
        "            \"research\": research_results,\n",
        "            \"guide\": guide_result.final_output_as(PreparationGuide)\n",
        "        }\n",
        "    \n",
        "    async def perform_research(self, query: str) -> str:\n",
        "        \"\"\"Perform a single research query.\"\"\"\n",
        "        try:\n",
        "            result = await Runner.run(\n",
        "                self.research_agent,\n",
        "                f\"Research this topic: {query}\"\n",
        "            )\n",
        "            return str(result.final_output)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Research failed for '{query}': {e}\")\n",
        "            return None\n",
        "    \n",
        "    async def prepare_for_date(self, date: str) -> List[dict]:\n",
        "        \"\"\"Prepare for all events on a specific date.\"\"\"\n",
        "        print(f\"📆 Fetching calendar events for {date}...\")\n",
        "        \n",
        "        # Fetch calendar events\n",
        "        calendar_result = await Runner.run(\n",
        "            self.calendar_agent,\n",
        "            f\"Get all my calendar events for {date}. Include event titles, times, locations, attendees, and descriptions.\"\n",
        "        )\n",
        "        \n",
        "        events_text = str(calendar_result.final_output)\n",
        "        \n",
        "        # Parse events (simple parsing - in production, you'd want more robust parsing)\n",
        "        if \"no events\" in events_text.lower() or \"empty\" in events_text.lower():\n",
        "            print(\"No events found for this date.\")\n",
        "            return []\n",
        "        \n",
        "        # Process each event\n",
        "        # For simplicity, we'll process the entire calendar output as one block\n",
        "        # In production, you'd parse individual events\n",
        "        results = []\n",
        "        result = await self.research_event(events_text)\n",
        "        results.append(result)\n",
        "        \n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Usage - Research Today's Events\n",
        "\n",
        "Let's use our Calendar Research Assistant to prepare for today's events. The orchestrator will:\n",
        "\n",
        "1. Connect to your Google Calendar\n",
        "2. Fetch all events for today\n",
        "3. Analyze each event and determine research needs\n",
        "4. Perform web searches on relevant topics\n",
        "5. Generate a comprehensive preparation guide\n",
        "\n",
        "> **Note**: This example uses real calendar data and performs actual web searches, so results will vary based on your calendar and current information available online.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗓️ Preparing for events on: 2025-09-16\n",
            "📆 Fetching calendar events for 2025-09-16...\n",
            "\n",
            "📅 Analyzing event...\n",
            "🔍 Researching 5 topics...\n",
            "📝 Creating preparation guide...\n",
            "\n",
            "============================================================\n",
            "EVENT 1 PREPARATION GUIDE\n",
            "============================================================\n",
            "\n",
            "📌 Event: Talk on OpenAI Agents SDK\n",
            "⏰ Time: 2025-09-16 8:00 PM – 9:00 PM America/Los_Angeles\n",
            "\n",
            "📋 Summary:\n",
            "A 60-minute developer-focused presentation that introduces OpenAI’s Agents SDK and the Responses API, with a practical walkthrough of single- and multi-agent patterns, tools/actions integration, state and memory, observability and guardrails, deployment and rate limits, security and data handling, plus positioning vs LangChain, AutoGen, and Semantic Kernel. Includes short Python and JavaScript demos with clear migration guidance from Assistants API.\n",
            "\n",
            "🔑 Key Insights:\n",
            "  • OpenAI introduced the Responses API (Mar 2025) to unify chat-style generation with tool use; the Assistants API is planned for deprecation by mid-2026 per OpenAI’s announcement, with migration guidance forthcoming.\n",
            "  • Open-source Agents SDK (Python and JS/TS) simplifies orchestration for single- and multi-agent workflows: configurable agents, agent handoffs, safety guardrails, and execution tracing.\n",
            "  • Built-in tools include web search, file search, computer use, code execution, and image generation; the SDKs also support wrapping your own functions as tools and using agents as tools.\n",
            "\n",
            "✅ Top Preparation Actions:\n",
            "  [P0 - MUST] Lock talk goals and audience profile; confirm format (in-person/virtual), runtime, and Q&A window (20 min)\n",
            "  [P0 - MUST] Finalize slide deck with clear narrative and visuals of architecture, tool flows, and tracing spans (60 min)\n",
            "  [P0 - MUST] Prepare Python demo: quickstart, one custom function tool, and simple multi-agent handoff (90 min)\n",
            "\n",
            "💬 Suggested Talking Points:\n",
            "  • Why agents now: Responses API unifies chat and tool use; Agents SDK provides a simple, composable way to orchestrate single- and multi-agent systems.\n",
            "  • Core mental model: Agent with instructions + tools; event loop plans, calls tools, and synthesizes outputs; agents can hand off to other agents.\n",
            "  • Python quickstart: create Agent(name, instructions), add a function tool, run via Runner; show a short end-to-end example.\n",
            "\n",
            "❓ Questions to Ask:\n",
            "  • What languages and runtimes does this audience primarily use (Python, JS/TS, .NET)?\n",
            "  • Do you need multi-agent orchestration now, or is a single-agent with tools sufficient?\n",
            "  • Which tools matter most for your use case (web search, file search, code execution, computer use, image generation, custom APIs)?\n"
          ]
        }
      ],
      "source": [
        "# Create the orchestrator\n",
        "orchestrator = CalendarResearchOrchestrator()\n",
        "\n",
        "# Get today's date\n",
        "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "print(f\"🗓️ Preparing for events on: {today}\")\n",
        "\n",
        "# Run the preparation workflow\n",
        "async def run_preparation():\n",
        "    results = await orchestrator.prepare_for_date(today)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"\\n✨ No events to prepare for today!\")\n",
        "        return\n",
        "    \n",
        "    # Display results for each event\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVENT {i} PREPARATION GUIDE\")\n",
        "        print('='*60)\n",
        "        \n",
        "        guide = result['guide']\n",
        "        \n",
        "        print(f\"\\n📌 Event: {guide.event_title}\")\n",
        "        print(f\"⏰ Time: {guide.event_time}\")\n",
        "        print(f\"\\n📋 Summary:\\n{guide.summary}\")\n",
        "        \n",
        "        print(\"\\n🔑 Key Insights:\")\n",
        "        for insight in guide.key_insights[:3]:  # Show top 3 insights\n",
        "            print(f\"  • {insight}\")\n",
        "        \n",
        "        print(\"\\n✅ Top Preparation Actions:\")\n",
        "        for item in guide.preparation_checklist[:3]:  # Show top 3 actions\n",
        "            print(f\"  [{item.priority.upper()}] {item.action} ({item.time_estimate})\")\n",
        "        \n",
        "        print(\"\\n💬 Suggested Talking Points:\")\n",
        "        for point in guide.talking_points[:3]:  # Show top 3 talking points\n",
        "            print(f\"  • {point}\")\n",
        "        \n",
        "        print(\"\\n❓ Questions to Ask:\")\n",
        "        for question in guide.questions_to_ask[:3]:  # Show top 3 questions\n",
        "            print(f\"  • {question}\")\n",
        "\n",
        "# Run the async function\n",
        "await run_preparation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Research Events for a Specific Date\n",
        "\n",
        "You can also prepare for events on any specific date. This is useful for:\n",
        "- Preparing for tomorrow's meetings\n",
        "- Getting ready for important events later in the week\n",
        "- Reviewing past events for follow-ups\n",
        "\n",
        "Simply provide the date in YYYY-MM-DD format:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗓️ Preparing for tomorrow's events: 2025-09-17\n",
            "📆 Fetching calendar events for 2025-09-17...\n",
            "\n",
            "📅 Analyzing event...\n",
            "🔍 Researching 5 topics...\n",
            "📝 Creating preparation guide...\n",
            "\n",
            "📊 Found 1 event(s) to prepare for tomorrow\n",
            "\n",
            "📅 OpenAI Agents SDK Event\n",
            "   Type: Product webinar / launch event\n",
            "   Research topics: 5\n",
            "   Preparation items: 11\n",
            "   Time needed: ~105 minutes\n"
          ]
        }
      ],
      "source": [
        "# Prepare for tomorrow's events\n",
        "tomorrow = (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "print(f\"🗓️ Preparing for tomorrow's events: {tomorrow}\")\n",
        "\n",
        "async def prepare_tomorrow():\n",
        "    results = await orchestrator.prepare_for_date(tomorrow)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"\\n✨ No events to prepare for tomorrow!\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n📊 Found {len(results)} event(s) to prepare for tomorrow\")\n",
        "    \n",
        "    # Show a brief summary for each event\n",
        "    for result in results:\n",
        "        guide = result['guide']\n",
        "        analysis = result['analysis']\n",
        "        \n",
        "        print(f\"\\n📅 {guide.event_title}\")\n",
        "        print(f\"   Type: {analysis.event_type}\")\n",
        "        print(f\"   Research topics: {len(analysis.research_queries)}\")\n",
        "        print(f\"   Preparation items: {len(guide.preparation_checklist)}\")\n",
        "        print(f\"   Time needed: ~{sum([15 if 'high' in item.priority.lower() else 10 if 'medium' in item.priority.lower() else 5 for item in guide.preparation_checklist])} minutes\")\n",
        "\n",
        "# Run the preparation for tomorrow\n",
        "await prepare_tomorrow()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Configuration: Custom Research Focus\n",
        "\n",
        "You can customize the research behavior for specific types of events or domains. This example shows how to create a specialized agent for technical meetings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Preparing for technical meeting...\n",
            "\n",
            "📊 Technical Preparation Summary:\n",
            "Below is a compact, engineering-first prep pack you can drop into the meeting. It emphasizes current best practices, concrete design options, code you can paste, performance/cost levers, and what’s changed recently (as of September 17, 2025, US).\n",
            "\n",
            "Meeting context\n",
            "- Meeting: AI Engineering Architecture Review\n",
            "- Time: 2:00–3:00 PM\n",
            "- Attendees: CTO, Lead Engineers, ML Team\n",
            "- Agenda: RAG architecture, vector databases, LLM deployment strategies\n",
            "\n",
            "Executive summary (recommended stance)\n",
            "- RAG: Adopt a two-track retrieval strategy: (1) baseline hybrid RAG (dense + keyword) with multi-stage reranking for QA/chat and (2) graph-enabled RAG for discovery/analytics at corpus scale (GraphRAG family), selecting “LazyGraphRAG” when you need global reasoning but can’t afford heavy pre-index summarization. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "- Vector DB: For managed, low-ops production with strict SLAs/compliance, Pinecone Serverless or BYOC; for feature-rich OSS with hybrid, quantization, and multi-tenancy knobs, Weaviate; for lowest-cost serverless and OSS/dev velocity, Chroma Cloud/OSS. Decide per-application based on access control model, multi-tenancy scale, retrieval style (hybrid, sparse, multi-vector), and cost model (RU/dimensions/GB). ([docs.pinecone.io](https://docs.pinecone.io/docs/architecture?utm_source=openai))\n",
            "- LLM deployment: Use cloud APIs (OpenAI/Anthropic/Gemini) for bursty/variable workloads and advanced features like prompt caching and web grounding; self-host (vLLM/TGI via Ray Serve/KServe) for steady load and strict data locality; combine with on-prem embeddings/rerank in sensitive domains. ([openai.com](https://openai.com/api/pricing))\n",
            "- Cost: Biggest levers are reranking (reduce context), prompt caching, smaller/cheaper models for easy turns (router), vector compression and hybrid search thresholds, and selective graph/global search. Expect 30–50% prompt cost reductions with caching/batch and >4× vector storage reductions with quantization at ~98–99% recall (with the right scheme). ([openai.com](https://openai.com/index/api-prompt-caching/?utm_source=openai))\n",
            "- What’s new: GraphRAG 1.0 ergonomics + LazyGraphRAG; Pinecone BYOC + backups/audit logs; Weaviate 1.25–1.31 adds named vectors, scalar quantization, ACORN filtered search, dynamic index; Chroma Cloud GA with clear usage pricing; Cohere Rerank v3.5; Gemini 2.0 pricing + web grounding; vLLM platform expansion. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/?utm_source=openai))\n",
            "\n",
            "1) Latest RAG architecture patterns and best practices\n",
            "- Retrieval core\n",
            "  - Hybrid retrieval = dense vector + keyword/BM25; fuse scores (alpha-weighting, relative score fusion). Improves robustness to lexical constraints/negation and can enforce must-have term filters; supported natively in Weaviate. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "  - Multi-stage rerank: retrieve top-k (e.g., 50–200), rerank with cross-encoder (Cohere Rerank v3.5 or BGE reranker), pass top m (e.g., 5–20) to LLM. This is a top cost/quality lever. ([docs.cohere.com](https://docs.cohere.com/v2/changelog/rerank-v3.5?utm_source=openai))\n",
            "  - Query transforms: multi-query expansion, HyDE, disambiguation; long documents: “situated embeddings” and late chunking to encode local context while keeping chunk granularity small. ([arxiv.org](https://arxiv.org/abs/2508.01959?utm_source=openai))\n",
            "- Graph-enabled RAG\n",
            "  - GraphRAG 1.0 and successors: build or derive a knowledge graph and community summaries for global questions; DRIFT and dynamic community selection cut token spend 70%+ vs naive global, while keeping quality. For lower upfront indexing cost, LazyGraphRAG matches or beats vector RAG on local queries and significantly reduces global query token costs. Use for analytics/exploration across large corpora. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/?utm_source=openai))\n",
            "- Serving/concurrency\n",
            "  - Treat RAG as a distributed system: schedule and autoscale per-stage (retriever, reranker, LLM), with continuous batching for LLMs and backpressure. Patchwork/RAGO papers show unified RAG-serving frameworks can boost throughput 1.5–2× and cut TTFT ~55%. Use vLLM/TGI behind a router. ([arxiv.org](https://arxiv.org/abs/2505.07833?utm_source=openai))\n",
            "- Security/governance\n",
            "  - Prefer retrieval “in place” with per-tenant isolation and row/field-level filters; bind access controls at query time. If centralizing embeddings, enforce multi-tenancy isolation and audit/access logs in the vector DB; Pinecone Enterprise + BYOC and Weaviate Cloud/BYOC both support enterprise controls. ([docs.pinecone.io](https://docs.pinecone.io/release-notes/2025?utm_source=openai))\n",
            "- Evaluation\n",
            "  - Use LLM- and rule-based evals: faithfulness, relevance, grounding rate, and answer quality; LlamaIndex evaluation modules and external libraries (DeepEval, RAGAS) help operationalize. Add business metrics (deflection, time saved) and track retrieval hit-rate and rerank acceptance. ([ts.llamaindex.ai](https://ts.llamaindex.ai/docs/llamaindex/modules/evaluation?utm_source=openai))\n",
            "\n",
            "2) Vector database comparison (Pinecone vs Weaviate vs Chroma)\n",
            "- Pinecone\n",
            "  - What it is: Fully managed serverless vector DB; serverless indexes backed by object storage; separate control vs data planes; read/write scale independently. Supports dense and sparse indexes, RBAC/SSO, audit logs (Enterprise), backup/restore APIs, BYOC (AWS/GCP preview) for data sovereignty, and an Assistant layer. Pricing is usage-based (storage, read/write units). Best when you need SLAs, ops offload, and compliance. ([docs.pinecone.io](https://docs.pinecone.io/docs/architecture?utm_source=openai))\n",
            "  - Notables: Serverless architecture optimization in 2025 for agentic workloads; audit logs and Admin API; BYOC preview; backups/restore. ([docs.pinecone.io](https://docs.pinecone.io/release-notes/2025?utm_source=openai))\n",
            "- Weaviate\n",
            "  - What it is: OSS + managed cloud; native hybrid (BM25+vector) with fusion, named vectors, dynamic index (flat→HNSW), ACORN-inspired filtered search, scalar/product/binary quantization, multi-tenancy with hot/warm/cold tiers. Serverless pricing based on stored dimensions; enterprise has AI Units with tiered storage. Best when you need algorithmic knobs, hybrid, and MT at scale. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "  - Notables: 1.25–1.31 added dynamic index, vector compression (SQ/RQ/PQ), multi-target vector search, async replication; docs include HNSW tuning and ANN benchmarks. ([weaviate.io](https://weaviate.io/developers/weaviate/release-notes/older-releases/release_1_25?utm_source=openai))\n",
            "- Chroma\n",
            "  - What it is: OSS developer-friendly vector DB with serverless Chroma Cloud. Features: vector + full-text + metadata filters, simple Python/TS APIs, and clear cloud pricing: storage per GiB-month, write per GiB, query scanning and egress-based. Good for cost-sensitive workloads, experimentation, and when OSS-first is preferred. SOC 2 on paid plans. ([trychroma.com](https://www.trychroma.com/pricing?utm_source=openai))\n",
            "Key trade-offs (quick guidance)\n",
            "- Security/Compliance: Pinecone Enterprise/BYOC > Weaviate Enterprise/BYOC > Chroma Cloud/OSS. ([docs.pinecone.io](https://docs.pinecone.io/release-notes/2025?utm_source=openai))\n",
            "- Hybrid and compression control: Weaviate strongest (native hybrid and multiple quantizers with high-recall recipes). Pinecone supports sparse/dense separately; Chroma emphasizes simplicity and price. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "- Multi-tenancy: Weaviate’s tenant shards and lifecycle (ACTIVE/INACTIVE/OFFLOADED) are deep; Pinecone has namespaces and inter-tenant isolation; Chroma has projects/databases per team. ([weaviate.io](https://weaviate.io/developers/weaviate/manage-data/multi-tenancy?utm_source=openai))\n",
            "- Pricing model simplicity: Chroma Cloud (GB + scan) and Weaviate Serverless (dimensions) are easier to model than Pinecone’s RUs; Pinecone gives highest enterprise assurances. ([trychroma.com](https://www.trychroma.com/pricing?utm_source=openai))\n",
            "\n",
            "3) LLM deployment options (self-hosted vs API)\n",
            "- When to favor API\n",
            "  - Spiky/variable traffic, SLOs on turnaround; exploit prompt caching/batch, web grounding, and latest models without infra maintenance (OpenAI GPT‑4o/o4-mini, Anthropic Claude Sonnet 4/Haiku 3.5, Google Gemini 2.0). ([openai.com](https://openai.com/api/pricing))\n",
            "- When to favor self-host\n",
            "  - Steady high QPS, strict data residency (air-gapped/VPC), custom decoding/quantization, LoRA multiplexing. Stack: vLLM or TGI behind Ray Serve; KServe if you want K8s-native and OpenAI-compatible ingress; quantization (INT4/8, FP8) and speculative decoding. ([docs.vllm.ai](https://docs.vllm.ai/en/latest?utm_source=openai))\n",
            "- Example topologies\n",
            "  - Ray Serve LLMRouter -> vLLM engines (multi-model, OpenAI-compatible), autoscale per model; or KServe vLLM runtime with KEDA autoscaling and token-based SLOs. ([docs.ray.io](https://docs.ray.io/en/latest/serve/llm/serving-llms.html?utm_source=openai))\n",
            "- Pricing signals (as of Sep 2025)\n",
            "  - OpenAI: GPT‑4o input ~$5/M tokens, output ~$20/M; prompt caching discounts on supported models. Gemini 2.0 Flash from $0.15/M input; Flash Lite lower; web grounding billed per 1k requests. Anthropic Sonnet 4 $3/M input, $15/M output, cache writes/hits tiered. Use batch APIs (-50%). ([openai.com](https://openai.com/api/pricing))\n",
            "\n",
            "4) Cost optimization strategies (practical levers)\n",
            "- Reduce prompt/context tokens\n",
            "  - Multi-stage rerank to pass fewer, better chunks; target m≈5–15. Cross-encoders typically pay for themselves. ([docs.cohere.com](https://docs.cohere.com/v2/changelog/rerank-v3.5?utm_source=openai))\n",
            "  - Prompt caching: OpenAI auto-applies 50% discount on cached input; Anthropic explicit cache control with 5-min/1-hour tiers (cache hits 10% of base input price). Batch for another 50% on both ends. ([openai.com](https://openai.com/index/api-prompt-caching/?utm_source=openai))\n",
            "  - Prompt compression: LLMLingua‑2 shows 2–5× compression with 1.6–2.9× end-to-end speedups; consider for large system/tool prompts. ([arxiv.org](https://arxiv.org/abs/2403.12968?utm_source=openai))\n",
            "- Route by difficulty\n",
            "  - Use intelligent routing: cheap/fast models for straightforward Qs; escalate to larger models for complex turns. Gemini, Bedrock, and in-house routers reduce cost up to ~30%. ([aws.amazon.com](https://aws.amazon.com/bedrock/pricing/?utm_source=openai))\n",
            "- Vector cost\n",
            "  - Quantization: Weaviate RQ/SQ/PQ; typical 4× memory reduction with ~98–99% recall for RQ; PQ needs tuning. Use rescoring with original vectors. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/starter-guides/managing-resources/compression?utm_source=openai))\n",
            "  - Hybrid thresholds: apply max vector distance cutoffs and required-term lexical filters to avoid off-topic candidates and shrink m. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "  - Pricing models:\n",
            "    - Weaviate Serverless: ~$0.095 per 1M dimensions-month; dimension-aware planning is predictable. ([weaviate.io](https://weaviate.io/pricing/serverless?utm_source=openai))\n",
            "    - Chroma Cloud: $0.33/GiB-month stored; $2.50/GiB written; query billed by TiB scanned and GiB returned. Great for massive cold tiers with low QPS. ([trychroma.com](https://www.trychroma.com/pricing?utm_source=openai))\n",
            "    - Pinecone Serverless: storage + read/write units; use backup/restore and BYOC to optimize infra spend and compliance. ([pinecone.io](https://www.pinecone.io/pricing/?utm_source=openai))\n",
            "- Serving efficiency\n",
            "  - Self-host: continuous batching (vLLM/TGI), KV-cache reuse/prefix caching, speculative decoding; consider INT4/8/FP8 quantization, and chunked prefill. ([docs.vllm.ai](https://docs.vllm.ai/en/latest?utm_source=openai))\n",
            "\n",
            "5) Recent developments you should know (Q2–Q3 2025)\n",
            "- GraphRAG: GraphRAG 1.0 ergonomics; DRIFT/dynamic global search; LazyGraphRAG cuts indexing and query costs for global+local. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/?utm_source=openai))\n",
            "- Weaviate: 1.25–1.31 adds dynamic index, scalar quantization, ACORN-style filtered search, async replication, named vectors/hybrid fusions; strengthened MT lifecycle. ([weaviate.io](https://weaviate.io/developers/weaviate/release-notes/older-releases/release_1_25?utm_source=openai))\n",
            "- Pinecone: BYOC (AWS/GCP previews), audit logs, backup/restore APIs, optimized serverless architecture for agentic workloads, Assistant GA with MCP integration. ([docs.pinecone.io](https://docs.pinecone.io/release-notes/2025?utm_source=openai))\n",
            "- Chroma Cloud: Clear usage-based pricing and SOC 2 on Team; Cloud for serverless vector/full-text/metadata. ([trychroma.com](https://www.trychroma.com/pricing?utm_source=openai))\n",
            "- Rerank/Embeddings: Cohere Rerank v3.5; Embed v4 with multimodal “Matryoshka” dims. ([docs.cohere.com](https://docs.cohere.com/v2/changelog/rerank-v3.5?utm_source=openai))\n",
            "- LLM APIs: Gemini 2.0 pricing & web grounding charges; OpenAI prompt caching and pricing updates; Anthropic Sonnet 4 and long-context billing over 200k input tokens. ([cloud.google.com](https://cloud.google.com/vertex-ai/generative-ai/pricing?utm_source=openai))\n",
            "- Self-host: vLLM continues to expand (speculative decoding, multi-LoRA, OpenAI-compatible API); Ray Serve integrates an LLM router and vLLM server components; KServe v0.13+ has vLLM backend and OpenAI protocol ingress. ([docs.vllm.ai](https://docs.vllm.ai/en/latest?utm_source=openai))\n",
            "\n",
            "Reference architectures (RAG)\n",
            "- Baseline hybrid RAG (answers/questions)\n",
            "  - Ingest: chunk (semantic/structure-aware), embed, store metadata (ACLs, source, timestamps).\n",
            "  - Retrieve: hybrid query (alpha≈0.5 start), K=100.\n",
            "  - Rerank: cross-encoder to top m≈10 with score threshold; add must-have filters.\n",
            "  - Generate: small/fast model default, escalate to larger model if uncertainty high.\n",
            "  - Cache: prompt caching + semantic response caching keyed by (intent, tenant, hash of top contexts).\n",
            "- Graph-enabled (discovery/analytics)\n",
            "  - Construct graph (entity/rel extraction) + community summaries; use LazyGraphRAG when avoiding heavy upfront summarization; DRIFT/dynamic selection for global; revert to local vector RAG for “who/what/when/where” questions. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/?utm_source=openai))\n",
            "\n",
            "Security checklist (vector + RAG)\n",
            "- Tenancy: map tenants to Weaviate tenants or Pinecone namespaces; enable audit logs; enforce RBAC and private networking; BYOC when data can’t leave VPC. ([weaviate.io](https://weaviate.io/developers/weaviate/manage-data/multi-tenancy?utm_source=openai))\n",
            "- Retrieval-time filtering: add ACL filters to hybrid/BM25; require-term filters for sensitive terms (Pinecone adds lexical filters for required terms). ([docs.pinecone.io](https://docs.pinecone.io/release-notes/2025?utm_source=openai))\n",
            "- PII: pre-index redaction/tokenization; keep PII in source systems and retrieve links/IDs rather than raw text when possible.\n",
            "\n",
            "Code you can paste (minimal, battle-tested patterns)\n",
            "- Hybrid + rerank (Weaviate, Python)\n",
            "  - Dense+BM25 hybrid search with a rerank pass; adjust alpha and max_vector_distance to tune noise vs recall.\n",
            "  - See Weaviate doc snippets for client v4 patterns. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/search/hybrid?utm_source=openai))\n",
            "- Pinecone Serverless + Cohere Rerank (Node/TS)\n",
            "  - Upsert with metadata, query top-k, call Cohere Rerank v3.5, take top m; pin to namespace/tenant; use Admin API for service accounts. ([docs.pinecone.io](https://docs.pinecone.io/docs/architecture?utm_source=openai))\n",
            "- Chroma Cloud (Python)\n",
            "  - Create DB, insert docs with metadata, query with where/where_document filters; use pricing knobs (GB written/stored, scan). ([github.com](https://github.com/chroma-core/chroma/blob/main/docs/docs.trychroma.com/pages/guides/index.md?utm_source=openai))\n",
            "- Self-host LLM (Ray Serve + vLLM)\n",
            "  - Deploy a multi-model OpenAI-compatible endpoint; autoscale by concurrent tokens; add speculative decoding + INT8 quantized weights if possible. ([docs.ray.io](https://docs.ray.io/en/latest/serve/llm/serving-llms.html?utm_source=openai))\n",
            "If you want, I can supply ready-to-run code blocks for each stack you’re considering (Python + TS), including docker-compose for Weaviate and Ray Serve manifests.\n",
            "\n",
            "Benchmarking and acceptance\n",
            "- Offline: retrieval recall@k, mAP; rerank quality (BEIR subsets) + latency; embedding ablations (BGE-M3 vs OpenAI/Cohere/Nomic). ([huggingface.co](https://huggingface.co/Enno-Ai/bge-m3?utm_source=openai))\n",
            "- E2E: faithfulness/grounding rate, answer helpfulness (LLM-judge + spot human eval), token cost per resolved question, p95 latency. Use LlamaIndex/DeepEval modules to automate. ([ts.llamaindex.ai](https://ts.llamaindex.ai/docs/llamaindex/modules/evaluation?utm_source=openai))\n",
            "- Serving: TTFT and tokens/sec for typical prompt/response lengths; demonstrate prompt caching hit rate and batch utilization.\n",
            "\n",
            "Decision guide (what to decide in this meeting)\n",
            "- Retrieval stack per use case:\n",
            "  - Helpdesk/chat: Weaviate or Pinecone + hybrid + rerank; Chroma Cloud for cost-minimal pilots.\n",
            "  - Research/analytics: GraphRAG path (start with LazyGraphRAG); keep vector RAG for local queries. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/?utm_source=openai))\n",
            "- Vector DB selection per tenant/security and pricing model (narrow to 1 primary + 1 backup).\n",
            "- LLM path: API-first with router and caching, plus a self-hosted lane (vLLM on Ray) for sensitive workloads and fixed-load endpoints.\n",
            "- Cost SLOs: target <$X per 1k Q&A turns at p95<Y s; instrument with caching hit-rate ≥ Z%.\n",
            "\n",
            "Appendix: concrete sources and updates to skim (team can open in meeting)\n",
            "- GraphRAG 1.0 + LazyGraphRAG + DRIFT/dynamic community selection. ([microsoft.com](https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/?utm_source=openai))\n",
            "- Pinecone architecture/pricing/release notes (BYOC, backups, audit logs). ([docs.pinecone.io](https://docs.pinecone.io/docs/architecture?utm_source=openai))\n",
            "- Weaviate hybrid search, releases 1.25–1.31, serverless pricing. ([docs.weaviate.io](https://docs.weaviate.io/weaviate/concepts/search/hybrid-search?utm_source=openai))\n",
            "- Chroma Cloud pricing/features. ([trychroma.com](https://www.trychroma.com/pricing?utm_source=openai))\n",
            "- Cohere Rerank v3.5 + deprecations. ([docs.cohere.com](https://docs.cohere.com/v2/changelog/rerank-v3.5?utm_source=openai))\n",
            "- OpenAI/Anthropic/Gemini pricing and prompt caching. ([openai.com](https://openai.com/api/pricing))\n",
            "- vLLM docs and Ray Serve LLM. ([docs.vllm.ai](https://docs.vllm.ai/en/latest?utm_source=openai))\n",
            "\n",
            "If you want detailed code kits (ready-to-run repos) for any of the stacks above or a cost spreadsheet with your volumes, say the word and I’ll tailor them to your data sizes and target SLOs.\n"
          ]
        }
      ],
      "source": [
        "# Create a specialized technical meeting preparation agent\n",
        "technical_prep_agent = Agent(\n",
        "    name=\"TechnicalMeetingPrep\",\n",
        "    instructions=\"\"\"You are a technical meeting preparation specialist focusing on engineering discussions.\n",
        "    \n",
        "    For technical meetings, emphasize:\n",
        "    - Latest technology trends and updates\n",
        "    - Code examples and implementation details\n",
        "    - Performance benchmarks and comparisons\n",
        "    - Security considerations\n",
        "    - Best practices and architectural patterns\n",
        "    - Open source alternatives\n",
        "    - Recent blog posts and documentation updates\n",
        "    \n",
        "    Generate highly technical, detailed preparation materials suitable for engineering discussions.\"\"\",\n",
        "    model=\"gpt-5\",\n",
        "    model_settings=ModelSettings(\n",
        "        reasoning=Reasoning(effort=\"high\"),\n",
        "    ),\n",
        "    tools=[WebSearchTool()],\n",
        ")\n",
        "\n",
        "# Example: Analyze a technical meeting\n",
        "async def prepare_technical_meeting():\n",
        "    event_description = \"\"\"\n",
        "    Meeting: AI Engineering Architecture Review\n",
        "    Time: 2:00 PM - 3:00 PM\n",
        "    Attendees: CTO, Lead Engineers, ML Team\n",
        "    Agenda: Discuss RAG architecture, vector databases, and LLM deployment strategies\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🔧 Preparing for technical meeting...\\n\")\n",
        "    \n",
        "    # Run the technical preparation\n",
        "    result = await Runner.run(\n",
        "        technical_prep_agent,\n",
        "        f\"\"\"Prepare comprehensive technical research for this meeting:\n",
        "        {event_description}\n",
        "        \n",
        "        Focus on:\n",
        "        1. Latest RAG architecture patterns and best practices\n",
        "        2. Vector database comparison (Pinecone vs Weaviate vs Chroma)\n",
        "        3. LLM deployment options (self-hosted vs API)\n",
        "        4. Cost optimization strategies\n",
        "        5. Recent developments in the field\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    print(\"📊 Technical Preparation Summary:\")\n",
        "    print(result.final_output)\n",
        "\n",
        "# Run the technical preparation\n",
        "await prepare_technical_meeting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "Congratulations! You've built a sophisticated Calendar Research Assistant using the OpenAI Agents SDK. Here's what we've accomplished:\n",
        "\n",
        "### 🎯 What We Built\n",
        "\n",
        "- **Calendar Integration** - Connected to Google Calendar via MCP connectors\n",
        "- **Multi-Agent System** - Orchestrated 4 specialized agents working together\n",
        "- **Parallel Processing** - Efficient research using async/await patterns\n",
        "- **Structured Outputs** - Type-safe results using Pydantic models\n",
        "- **Streaming Support** - Real-time updates for better user experience\n",
        "- **Customization** - Specialized agents for different meeting types\n",
        "\n",
        "### 🔧 Key Components\n",
        "\n",
        "1. **Calendar Agent** - Fetches events from Google Calendar\n",
        "2. **Event Analyzer** - Determines research needs for each event\n",
        "3. **Research Agent** - Performs web searches on relevant topics\n",
        "4. **Preparation Guide Agent** - Synthesizes findings into actionable guides\n",
        "5. **Orchestrator** - Coordinates the entire workflow\n",
        "\n",
        "### 💡 Best Practices Demonstrated\n",
        "\n",
        "- **Separation of Concerns** - Each agent has a specific, focused role\n",
        "- **Error Handling** - Graceful fallbacks when research fails\n",
        "- **Performance Optimization** - Parallel research queries\n",
        "- **User Experience** - Clear progress indicators and streaming\n",
        "- **Flexibility** - Easy to extend with new agents or tools\n",
        "\n",
        "### 🚀 Next Steps\n",
        "\n",
        "To extend this system, consider:\n",
        "\n",
        "1. **Add more connectors** - Slack, Email, Notion for richer context\n",
        "2. **Implement caching** - Store research results to avoid redundant searches\n",
        "3. **Add personalization** - Learn from user feedback and preferences\n",
        "4. **Create a UI** - Build a web interface using the streaming capabilities\n",
        "5. **Schedule automation** - Run preparation automatically each morning\n",
        "6. **Export options** - Generate PDFs or send summaries via email\n",
        "\n",
        "### 📚 Resources\n",
        "\n",
        "- [OpenAI Agents SDK Documentation](https://platform.openai.com/docs/agents)\n",
        "- [Model Context Protocol (MCP)](https://platform.openai.com/docs/guides/tools-connectors-mcp)\n",
        "- [Google Calendar API](https://developers.google.com/calendar)\n",
        "- [Pydantic Documentation](https://docs.pydantic.dev/)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
