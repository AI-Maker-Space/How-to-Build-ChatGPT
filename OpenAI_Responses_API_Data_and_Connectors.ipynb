{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4ab430",
   "metadata": {},
   "source": [
    "# OpenAI Responses API - Data and Connectors Demo\n",
    "\n",
    "Welcome to AI Makerspace! This notebook demonstrates the powerful **File Search** capability of the OpenAI Responses API, which allows models to retrieve information from your uploaded documents through semantic and keyword search.\n",
    "\n",
    "The File Search tool offers several key advantages:\n",
    "- **Semantic search** - Find relevant information by meaning, not just keywords\n",
    "- **Vector store integration** - Scalable knowledge base management\n",
    "- **Automatic citations** - Get references to source documents in responses\n",
    "- **Hosted solution** - No need to implement search infrastructure yourself\n",
    "- **Retrieval customization** - Control search results, filtering, and metadata\n",
    "- **Multiple file formats** - Support for PDFs, docs, code files, and more\n",
    "\n",
    "This notebook walks through setting up vector stores, uploading documents, and using file search with the Responses API to create AI applications that can reason over your data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c97ec0",
   "metadata": {},
   "source": [
    "### Setup and Authentication\n",
    "\n",
    "First, we need to set up our OpenAI API credentials. We'll use `getpass` to securely input the API key without exposing it in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f21995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca7137",
   "metadata": {},
   "source": [
    "Now we'll initialize the OpenAI client and import the additional libraries we'll need for file handling and requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba4bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a282edd",
   "metadata": {},
   "source": [
    "### Setting Up File Search: Vector Stores and File Upload\n",
    "\n",
    "Before we can use file search with the Responses API, we need to create a knowledge base. This involves three key steps:\n",
    "\n",
    "1. **Upload files** to the OpenAI File API\n",
    "2. **Create a vector store** to organize our knowledge base\n",
    "3. **Add files to the vector store** for semantic search\n",
    "\n",
    "Let's start by creating a helper function that can handle both local files and URLs, similar to the example in the OpenAI documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698db8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(client, file_path):\n",
    "    \"\"\"\n",
    "    Upload a file to OpenAI's File API.\n",
    "    Supports both local file paths and URLs.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        file_path: Path to local file or URL to download from\n",
    "        \n",
    "    Returns:\n",
    "        str: File ID from OpenAI\n",
    "    \"\"\"\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        print(f\"Downloading file from URL: {file_path}\")\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        print(f\"Uploading local file: {file_path}\")\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    \n",
    "    print(f\"File uploaded successfully. File ID: {result.id}\")\n",
    "    return result.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6699f21",
   "metadata": {},
   "source": [
    "Now let's upload your local PDF file about embeddings from the data folder. This will serve as our knowledge base for testing file search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55a2de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading local file: /home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\n",
      "File uploaded successfully. File ID: file-WrMJNKgKCN71PzTs2BQrE4\n"
     ]
    }
   ],
   "source": [
    "# Upload your local PDF file from the data folder\n",
    "file_id = create_file(client, \"/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ecc16",
   "metadata": {},
   "source": [
    "Next, we need to create a **vector store**. Think of this as a container that organizes your uploaded files for semantic search. The vector store will automatically process your documents and create embeddings for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17983ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n",
      "Vector Store ID: vs_68c1aea8ca8081918962708d7420ffde\n",
      "Name: AI_Makerspace_Knowledge_Base\n",
      "Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store for our knowledge base\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"AI_Makerspace_Knowledge_Base\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created successfully!\")\n",
    "print(f\"Vector Store ID: {vector_store.id}\")\n",
    "print(f\"Name: {vector_store.name}\")\n",
    "print(f\"Status: {vector_store.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5e7f7",
   "metadata": {},
   "source": [
    "Now we'll add our uploaded file to the vector store. This step tells OpenAI to process the document and make it available for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fa8d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File added to vector store successfully!\n",
      "File ID: file-WrMJNKgKCN71PzTs2BQrE4\n",
      "Status: in_progress\n",
      "Created at: 1757523628\n"
     ]
    }
   ],
   "source": [
    "# Add the uploaded file to our vector store\n",
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "\n",
    "print(f\"File added to vector store successfully!\")\n",
    "print(f\"File ID: {result.id}\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Created at: {result.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f55aa",
   "metadata": {},
   "source": [
    "We need to wait for the file processing to complete before we can use it for search. Let's check the status and wait until it's ready. The file needs to be in `completed` status for search to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8201da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File statuses: ['completed']\n",
      "✅ All files processed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_file_processing(client, vector_store_id, max_wait_time=300):\n",
    "    \"\"\"\n",
    "    Wait for all files in a vector store to be processed.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        vector_store_id: ID of the vector store to check\n",
    "        max_wait_time: Maximum time to wait in seconds (default: 5 minutes)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all files are completed, False if timeout\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        # Check the status of files in the vector store\n",
    "        files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "        \n",
    "        statuses = [file.status for file in files.data]\n",
    "        print(f\"File statuses: {statuses}\")\n",
    "        \n",
    "        if all(status == \"completed\" for status in statuses):\n",
    "            print(\"✅ All files processed successfully!\")\n",
    "            return True\n",
    "        elif any(status == \"failed\" for status in statuses):\n",
    "            print(\"❌ One or more files failed to process\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"⏳ Still processing... waiting 10 seconds\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    print(\"⏰ Timeout waiting for file processing\")\n",
    "    return False\n",
    "\n",
    "# Wait for our file to be processed\n",
    "wait_for_file_processing(client, vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfacb6",
   "metadata": {},
   "source": [
    "### Basic File Search with the Responses API\n",
    "\n",
    "Now that our knowledge base is set up, we can use the **file search** tool with the Responses API! This is where the magic happens - the model can automatically search through your documents and provide answers with citations.\n",
    "\n",
    "Key features of file search:\n",
    "- **Automatic tool calling** - The model decides when to search your files\n",
    "- **Semantic understanding** - Finds relevant information by meaning, not just keywords  \n",
    "- **Source citations** - Get references to specific documents in the response\n",
    "- **Multiple output types** - Both search calls and final messages with citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b5d2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Response from file search:\n",
      "==================================================\n",
      "Dense (single‑vector) embeddings have a fundamental capacity limit: because their representation power is bounded by the embedding dimension, they cannot realize all possible top‑k relevance combinations; for any fixed dimension d there exist queries/relevance patterns they will fail to return, no matter how they’re trained . This limitation can be formalized via sign‑rank: some relevance matrices require higher dimensionality than the model has, so dense embeddings cannot capture them exactly . As tasks demand more combinations (e.g., instruction‑based retrieval), the dimensionality needed grows rapidly and becomes impractical at scale .\n"
     ]
    }
   ],
   "source": [
    "# Use file search with the Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"🔍 Response from file search:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151b393",
   "metadata": {},
   "source": [
    "Let's examine the full response structure to understand what file search returns. The response contains multiple output items including the search call details and the final message with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e31037ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Full Response Analysis:\n",
      "==================================================\n",
      "\n",
      "📋 Output Item 1:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00540f081a081e58d4e59bff7ab047aaa5f6e571614\n",
      "\n",
      "📋 Output Item 2:\n",
      "   Type: file_search_call\n",
      "   ID: fs_68c1b008dea081a0a679634f7078f796047aaa5f6e571614\n",
      "   Status: completed\n",
      "   Queries: ['What is the main problem with dense vector embeddings?', 'limitations of dense vector embeddings main problem', 'dense embeddings problem opacity interpretability sparsity', 'dense vs sparse embeddings advantages disadvantages', 'What is the problem with dense vector representations in NLP?']\n",
      "\n",
      "📋 Output Item 3:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00b202881a0ace83f23d5e7eb18047aaa5f6e571614\n",
      "\n",
      "📋 Output Item 4:\n",
      "   Type: message\n",
      "   ID: msg_68c1b01ec16881a0b1554faa33adee5c047aaa5f6e571614\n",
      "   Role: assistant\n",
      "   Content items: 1\n",
      "   📚 Found 3 citations\n",
      "      Citation 1: Embedding-Based.pdf\n",
      "      Citation 2: Embedding-Based.pdf\n",
      "      Citation 3: Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Examine the full response structure\n",
    "print(\"📊 Full Response Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, output_item in enumerate(response.output):\n",
    "    print(f\"\\n📋 Output Item {i + 1}:\")\n",
    "    print(f\"   Type: {output_item.type}\")\n",
    "    print(f\"   ID: {output_item.id}\")\n",
    "    \n",
    "    if output_item.type == \"file_search_call\":\n",
    "        print(f\"   Status: {output_item.status}\")\n",
    "        print(f\"   Queries: {output_item.queries}\")\n",
    "        \n",
    "    elif output_item.type == \"message\":\n",
    "        print(f\"   Role: {output_item.role}\")\n",
    "        print(f\"   Content items: {len(output_item.content)}\")\n",
    "        \n",
    "        # Check for citations in the message content\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                print(f\"   📚 Found {len(content_item.annotations)} citations\")\n",
    "                for j, annotation in enumerate(content_item.annotations):\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        print(f\"      Citation {j + 1}: {annotation.filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4765a",
   "metadata": {},
   "source": [
    "### Including Search Results in the Response\n",
    "\n",
    "By default, the file search call doesn't return the actual search results - only the final answer with citations. However, you can include the search results using the `include` parameter. This is useful for understanding what information was retrieved and how the model used it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include search results in the response\n",
    "response_with_results = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What are the key limitations of dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🔍 Response with search results:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_with_results.output_text)\n",
    "\n",
    "# Examine the search results\n",
    "for output_item in response_with_results.output:\n",
    "    if output_item.type == \"file_search_call\" and hasattr(output_item, 'search_results'):\n",
    "        if output_item.search_results:\n",
    "            print(f\"\\n📄 Search Results Found:\")\n",
    "            print(f\"Number of results: {len(output_item.search_results)}\")\n",
    "            \n",
    "            for i, result in enumerate(output_item.search_results):\n",
    "                print(f\"\\nResult {i + 1}:\")\n",
    "                print(f\"  Score: {result.score}\")\n",
    "                print(f\"  Content preview: {result.content[:200]}...\")\n",
    "        else:\n",
    "            print(\"\\n📄 No search results returned (search_results is None)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395be437",
   "metadata": {},
   "source": [
    "### Retrieval Customization\n",
    "\n",
    "The file search tool offers several customization options to optimize performance and results:\n",
    "\n",
    "1. **Limiting results** - Control the number of search results to reduce tokens and latency\n",
    "2. **Metadata filtering** - Filter search results based on file attributes\n",
    "3. **Search quality vs. performance** - Balance between comprehensive results and response speed\n",
    "\n",
    "Let's explore these customization options:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df98e54",
   "metadata": {},
   "source": [
    "#### Limiting the Number of Results\n",
    "\n",
    "By default, file search may retrieve many results. You can limit this using `max_num_results` to reduce token usage and improve response time, though this may impact answer quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e82112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Limited Results Response:\n",
      "==================================================\n",
      "Deep research is an agent inside ChatGPT that autonomously performs multi‑step web research: it searches, reads, and analyzes hundreds of online sources, then synthesizes them into a documented, research‑analyst‑style report with citations and a summary of its reasoning . It’s powered by a version of OpenAI’s upcoming o3 model optimized for browsing and data analysis, and was trained on real‑world tasks requiring browser and Python tool use using the reinforcement‑learning methods behind OpenAI’s o1 reasoning model  .\n",
      "\n",
      "Key points:\n",
      "- What it’s for: intensive knowledge work (finance, science, policy, engineering) and any complex, niche information gathering; it’s also useful for high‑consideration shopping, with every claim cited for easy verification .\n",
      "- How it works: you select “deep research” in ChatGPT, enter your query (you can attach files), and it executes a multi‑step plan, showing a sidebar of steps and sources as it works .\n",
      "- Output and speed: returns a comprehensive report, and typically takes 5–30 minutes; OpenAI notes richer outputs like embedded images and visualizations are being added .\n",
      "- Availability: available to Pro users now, with Plus and Team to follow .\n",
      "- When to use it: choose deep research for deep, domain‑specific questions where verified detail matters; use GPT‑4o for fast, real‑time multimodal chats .\n"
     ]
    }
   ],
   "source": [
    "# Limit the number of search results\n",
    "response_limited = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 2  # Limit to 2 results for faster, more focused responses\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🎯 Limited Results Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_limited.output_text)\n",
    "\n",
    "# Show how many results were actually used\n",
    "for output_item in response_limited.output:\n",
    "    if output_item.type == \"file_search_call\":\n",
    "        if hasattr(output_item, 'search_results') and output_item.search_results:\n",
    "            print(f\"\\n📊 Used {len(output_item.search_results)} search results (limited to 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae0f6a",
   "metadata": {},
   "source": [
    "#### Practical Example: Adding Local Files to Your Knowledge Base\n",
    "\n",
    "Let's demonstrate how to add local files from your project to create a more comprehensive knowledge base. We'll check if there are any PDF files in the data directory and add them to our vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f1bd3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 1 PDF files in ./data:\n",
      "   - Embedding-Based.pdf\n",
      "\n",
      "📤 Uploading Embedding-Based.pdf...\n",
      "Uploading local file: ./data/Embedding-Based.pdf\n",
      "File uploaded successfully. File ID: file-3ExVmjLLS4qmjyyrHUDDMe\n",
      "✅ Added to vector store successfully!\n",
      "\n",
      "⏳ Waiting for new files to be processed...\n",
      "File statuses: ['in_progress', 'completed']\n",
      "⏳ Still processing... waiting 10 seconds\n",
      "File statuses: ['completed', 'completed']\n",
      "✅ All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check for local PDF files in the data directory\n",
    "data_dir = \"./data\"\n",
    "pdf_files = []\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    pdf_files = glob.glob(os.path.join(data_dir, \"*.pdf\"))\n",
    "    print(f\"📁 Found {len(pdf_files)} PDF files in {data_dir}:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"   - {os.path.basename(pdf_file)}\")\n",
    "else:\n",
    "    print(f\"📁 Data directory {data_dir} not found\")\n",
    "\n",
    "# Upload and add local files to the vector store\n",
    "local_file_ids = []\n",
    "for pdf_file in pdf_files[:1]:  # Let's add just the first PDF to avoid too much processing time\n",
    "    try:\n",
    "        print(f\"\\n📤 Uploading {os.path.basename(pdf_file)}...\")\n",
    "        local_file_id = create_file(client, pdf_file)\n",
    "        local_file_ids.append(local_file_id)\n",
    "        \n",
    "        # Add to vector store\n",
    "        result = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store.id,\n",
    "            file_id=local_file_id\n",
    "        )\n",
    "        print(f\"✅ Added to vector store successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {pdf_file}: {e}\")\n",
    "\n",
    "if local_file_ids:\n",
    "    print(f\"\\n⏳ Waiting for new files to be processed...\")\n",
    "    wait_for_file_processing(client, vector_store.id)\n",
    "else:\n",
    "    print(\"\\n💡 No local files were added. You can place PDF files in the ./data directory to test with your own documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdd3ac",
   "metadata": {},
   "source": [
    "### Multi-Document Search and Comparison\n",
    "\n",
    "Now that we have multiple documents in our knowledge base, we can ask questions that require searching across different sources. This demonstrates the power of semantic search for complex queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f992e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Complex Multi-Document Query:\n",
      "============================================================\n",
      "Below is a concise compare/contrast of the AI R&D approaches surfaced in your documents, with their core methodologies and benefits.\n",
      "\n",
      "Retrieval modeling paradigms\n",
      "- Dense single‑vector embeddings (aka dense retrieval)\n",
      "  - Methodology: Represent queries and documents with one vector each; retrieve via nearest neighbors. The paper analyzes representational limits using communication‑complexity tools (sign‑rank) and formalizes the minimum embedding dimension needed to realize top‑k relevance orderings; it also isolates capacity with “free embedding” optimization and a stress‑test dataset (LIMIT) to show empirical limits even for simple queries   .\n",
      "  - Benefits: Extremely fast and scalable first‑stage retrieval; strong generalization across many datasets and tasks when instruction‑tuned.\n",
      "  - Key limitation/trade‑off: Fixed dimensionality bounds the number of top‑k combinations a model can represent; as tasks demand more combinatorial relevance (e.g., instruction‑based or reasoning queries), some sets become unretrievable regardless of training data or optimizer  .\n",
      "\n",
      "- Cross‑encoders (rerankers)\n",
      "  - Methodology: Jointly encode query+document and score with a powerful LM; typically used to rerank a candidate set. In LIMIT, a long‑context reranker (Gemini‑2.5‑Pro) solved 100% of small‑set queries in a single pass, unlike best embedder baselines, but at higher compute cost, which prevents use as the first stage at large scale  .\n",
      "  - Benefits: High precision and expressiveness; can overcome the dimensionality limits that bind single‑vector models.\n",
      "  - Trade‑off: Much more computationally expensive; practical primarily for reranking.\n",
      "\n",
      "- Multi‑vector retrievers (e.g., ColBERT‑style)\n",
      "  - Methodology: Represent each sequence with multiple token‑level vectors and aggregate with MaxSim; increases expressiveness relative to single‑vector models. Performs substantially better than single‑vector baselines on LIMIT, even with a smaller backbone .\n",
      "  - Benefits: Retains sublinear retrieval properties while capturing richer matching patterns.\n",
      "  - Open question: Less explored for instruction‑following/reasoning tasks; transfer remains to be proven .\n",
      "\n",
      "- Sparse/lexical retrieval (e.g., BM25 or neural sparse)\n",
      "  - Methodology: High‑dimensional sparse term spaces (or neural variants) that behave like very high‑dimensional vectors.\n",
      "  - Benefits: Avoid some dense‑embedding limits due to effectively higher dimensionality; strong when lexical overlap exists .\n",
      "  - Trade‑off: Harder to apply to instruction‑following or abstract reasoning where little lexical overlap exists .\n",
      "\n",
      "Evaluation and stress‑testing as a research method\n",
      "- LIMIT dataset and capacity‑revealing metrics\n",
      "  - Methodology: Construct a dataset that maximizes the number of relevant top‑k combinations; analyze “qrel graph density” and “average query strength” to quantify how combinatorially demanding a retrieval task is. LIMIT exposes failure modes of dense embeddings and shows non‑correlation with familiar benchmarks like BEIR, indicating overfitting risks and hidden gaps in current evaluations  .\n",
      "  - Benefits: Gives principled, theory‑linked stress tests; guides when to switch from single‑vector embeddings to alternatives like cross‑encoders or multi‑vector models .\n",
      "\n",
      "Agentic, tool-using AI for research tasks\n",
      "- Deep research (agent trained with end‑to‑end RL for browsing and reasoning)\n",
      "  - Methodology: An agent that plans multi‑step trajectories on the web, backtracks, uses tools (browser, Python), works over user files, embeds graphs/images, and cites sources; trained with the same reinforcement‑learning style behind OpenAI’s reasoning models (o1), and powered by a browsing‑optimized o3 variant .\n",
      "  - Benefits: Produces comprehensive, verifiable work products; achieves new highs on realistic evaluations like Humanity’s Last Exam (26.6% accuracy) and tops GAIA’s external leaderboard, reflecting strength in real‑world, tool‑use‑heavy tasks . Compared to general chat models (e.g., GPT‑4o) optimized for real‑time multimodal dialogue, deep research excels when depth, documentation, and source‑grounding are required .\n",
      "  - Trade‑off: Willing to spend more runtime/compute per query to plan, browse, and verify; ideal when the task’s value justifies the cost.\n",
      "\n",
      "How these approaches complement each other\n",
      "- Scalability vs expressiveness: Single‑vector embeddings are ideal for large‑scale, low‑latency retrieval; as complexity rises, rerankers or multi‑vector methods recapture precision at higher cost, a pattern made explicit by LIMIT’s results and theory  .\n",
      "- Model training signal: Contrastive pretraining/instruction‑tuning for retrieval embeddings vs end‑to‑end reinforcement learning for agentic browsing and reasoning; the former optimizes static representations, the latter optimizes behavior over tools and time  .\n",
      "- Evaluation philosophy: Capacity‑revealing, theoretically‑motivated stress tests (LIMIT) for retrieval versus real‑world, tool‑use evaluations (GAIA, Humanity’s Last Exam) for agentic systems; together, they provide coverage from representational limits to practical task completion  .\n",
      "\n",
      "Practical takeaways\n",
      "- Use dense embeddings for fast first‑stage retrieval; add cross‑encoder reranking or multi‑vector retrievers as complexity and precision needs rise, especially for instruction/logic‑heavy queries highlighted by LIMIT  .\n",
      "- Incorporate capacity‑aware evaluation (graph density, LIMIT‑style combinations) to detect when single‑vector methods will fail and to justify switching architectures  .\n",
      "- For knowledge‑work deliverables that demand thorough sourcing and multi‑step web research, prefer an agentic RL‑trained system like deep research; it is explicitly optimized to browse, reason, and verify at SOTA levels on practical benchmarks   .\n",
      "\n",
      "📚 Documents Referenced:\n",
      "==============================\n",
      "📄 deep_research_blog.pdf\n",
      "📄 Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ask a complex question that might require multiple documents\n",
    "complex_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Compare and contrast different approaches to AI research and development mentioned in the documents. What are the key methodologies and their benefits?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🔬 Complex Multi-Document Query:\")\n",
    "print(\"=\" * 60)\n",
    "print(complex_response.output_text)\n",
    "\n",
    "# Show which documents were referenced\n",
    "print(\"\\n📚 Documents Referenced:\")\n",
    "print(\"=\" * 30)\n",
    "for output_item in complex_response.output:\n",
    "    if output_item.type == \"message\":\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                unique_files = set()\n",
    "                for annotation in content_item.annotations:\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        unique_files.add(annotation.filename)\n",
    "                \n",
    "                for filename in unique_files:\n",
    "                    print(f\"📄 {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fc7fb",
   "metadata": {},
   "source": [
    "### Supported File Formats\n",
    "\n",
    "The file search tool supports a wide variety of file formats. Here's a summary of what you can upload:\n",
    "\n",
    "**Text and Code Files:**\n",
    "- `.py`, `.js`, `.ts`, `.java`, `.cpp`, `.c`, `.cs`, `.go`, `.php`, `.rb`, `.sh`\n",
    "- `.html`, `.css`, `.json`, `.md`, `.txt`, `.tex`\n",
    "\n",
    "**Document Formats:**\n",
    "- `.pdf` - PDF documents\n",
    "- `.doc`, `.docx` - Microsoft Word documents  \n",
    "- `.pptx` - PowerPoint presentations\n",
    "\n",
    "**Requirements:**\n",
    "- For text files, encoding must be UTF-8, UTF-16, or ASCII\n",
    "- Files are processed automatically for semantic search\n",
    "- Maximum file size and token limits apply (check OpenAI documentation for current limits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1513aa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The OpenAI Responses API with File Search represents a powerful leap forward in building AI applications that can reason over your data. Key takeaways:\n",
    "\n",
    "✅ **Hosted solution** - No need to manage your own vector database or search infrastructure  \n",
    "✅ **Semantic search** - Find information by meaning, not just keywords  \n",
    "✅ **Automatic citations** - Get references to source documents in responses  \n",
    "✅ **Multiple file formats** - Support for PDFs, docs, code files, and more  \n",
    "✅ **Retrieval customization** - Control search results and performance  \n",
    "✅ **Easy integration** - Simple API that works seamlessly with the Responses API  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175ab8",
   "metadata": {},
   "source": [
    "# MCP (Model Context Protocol) and Connectors\n",
    "\n",
    "In addition to file search, the OpenAI Responses API supports **MCP (Model Context Protocol)** servers and **Connectors** that give models the ability to connect to and control external services. This section demonstrates:\n",
    "\n",
    "## Two Types of External Integrations:\n",
    "\n",
    "### 1. **Connectors** \n",
    "- OpenAI-maintained MCP wrappers for popular services\n",
    "- Pre-built integrations like Google Workspace, Dropbox, Microsoft 365\n",
    "- OAuth-based authentication\n",
    "- Hosted by OpenAI with guaranteed reliability\n",
    "\n",
    "### 2. **Remote MCP Servers**\n",
    "- Third-party servers implementing the MCP protocol\n",
    "- Can be any server on the public Internet\n",
    "- Custom tools and capabilities\n",
    "- Examples: GitHub, Stripe, custom business tools\n",
    "\n",
    "## Key Benefits:\n",
    "- **Automatic tool discovery** - Models learn available tools dynamically\n",
    "- **Approval workflows** - Control what data is shared with external services  \n",
    "- **Real-time capabilities** - Access live data and perform actions\n",
    "- **Extensible** - Connect to virtually any service with an MCP server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cf6fb",
   "metadata": {},
   "source": [
    "⚠️  **Please obtain a Google OAuth token first!**\n",
    "\n",
    "📝 **Steps to get a token:**\n",
    "1. Go to [Google OAuth Playground](https://developers.google.com/oauthplayground/)\n",
    "2. Enter scope: `https://www.googleapis.com/auth/calendar.events`\n",
    "3. Authorize APIs and exchange for token\n",
    "4. Replace the token in the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eab0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_OAUTH_TOKEN\"] = getpass.getpass(\"Enter your Google OAuth token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cad697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Google Calendar Response:\n",
      "==================================================\n",
      "Here’s what’s on your calendar for Friday, September 12 (times shown in your calendar’s timezone: America/Los_Angeles):\n",
      "\n",
      "- 10:00 AM–10:30 AM — GPU Engineering Meetups: Fundamentals of GPU Orchestration\n",
      "  - Location: https://luma.com/join/g-wfW1s6SgOuSmfwh\n",
      "  - Details: Overview of why GPU orchestration matters; how Ray, Horovod, and DeepSpeed work; consistency models; and real-world trade-offs. Hosted by Abi.\n",
      "  - More info: https://luma.com/event/evt-MierqIlbxNeBy87?pk=g-wfW1s6SgOuSmfwh\n",
      "  - Calendar link: https://www.google.com/calendar/event?eid=X2Nscjc4YmFkZDVpbjRzYTlkaGg3Z2pqNTg5c2pnZHEwY2xyNmFyamtlY242b3Q5ZWRsZ2cgY2hyaXNAYWxleGl1ay5jYQ\n",
      "  - Status: Busy\n",
      "\n",
      "Want me to add a reminder or pull in attendee/meeting details if available?\n"
     ]
    }
   ],
   "source": [
    "# Google Calendar Connector Example\n",
    "# Note: You'll need to obtain an OAuth access token from Google\n",
    "# For testing, use Google's OAuth 2.0 Playground: https://developers.google.com/oauthplayground/\n",
    "\n",
    "# Example OAuth scope for Google Calendar:\n",
    "# https://www.googleapis.com/auth/calendar.events\n",
    "\n",
    "def demonstrate_google_calendar_connector(token):\n",
    "    \"\"\"\n",
    "    Demonstrate Google Calendar Connector functionality.\n",
    "    This example shows how to query calendar events using the Responses API.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Query today's calendar events\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\",\n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "            }],\n",
    "            input=\"What events do I have on my Google Calendar for Friday, September 12th? Please provide details about each event including time, title, and any important information.\"\n",
    "        )\n",
    "        \n",
    "        print(\"📅 Google Calendar Response:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with Google Calendar Connector: {e}\")\n",
    "        return None\n",
    "\n",
    "# Demonstrate the connector (will show setup instructions if no token provided)\n",
    "calendar_response = demonstrate_google_calendar_connector(os.environ[\"GOOGLE_OAUTH_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a04f4",
   "metadata": {},
   "source": [
    "#### Advanced Google Calendar Operations\n",
    "\n",
    "The Google Calendar Connector supports several sophisticated operations that can be combined for powerful calendar-based workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92c871e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Advanced Calendar Analysis:\n",
      "==================================================\n",
      "Here’s your week-at-a-glance analysis based on your primary Google Calendar for Sep 8–14, 2025 (assumed timezone: America/Los_Angeles). If your timezone or workweek differs, tell me and I’ll re-run.\n",
      "\n",
      "1) Busiest days\n",
      "- Monday (Sep 8): 1 meeting, 30 minutes\n",
      "  - 10:30–11:00 — “Chris Alexiuk and Anthony Witherspoon”\n",
      "- Friday (Sep 12): 1 meeting, 30 minutes\n",
      "  - 10:00–10:30 — “GPU Engineering Meetups: Fundamentals of GPU Orchestration”\n",
      "- Tuesday, Wednesday, Thursday: No meetings\n",
      "\n",
      "2) Potential scheduling conflicts\n",
      "- None found. No overlaps or back-to-backs. Both events are short and separated by days.\n",
      "\n",
      "3) Recommendations for optimal meeting times\n",
      "Assumptions: standard working hours 9:00–17:00 PT.\n",
      "- Best days for meetings this week (widest availability):\n",
      "  - Tuesday (Sep 9): 9:00–12:00, 13:00–17:00 completely open\n",
      "  - Wednesday (Sep 10): 9:00–12:00, 13:00–17:00 completely open\n",
      "  - Thursday (Sep 11): 9:00–12:00, 13:00–17:00 completely open\n",
      "- Good windows on days with existing events:\n",
      "  - Monday (Sep 8): 9:00–10:30, 11:00–17:00\n",
      "  - Friday (Sep 12): 9:00–10:00, 10:30–17:00\n",
      "- Actionable scheduling tips\n",
      "  - If you’re proposing times to others: offer Tue–Thu 10:00–12:00 PT or 13:30–16:00 PT for best acceptance and focus.\n",
      "  - Cluster meetings: Since you already have brief items Mon and Fri mornings, consider slotting other external/admin meetings adjacent to those blocks to keep Tue–Thu more open for deep work.\n",
      "  - Hold protected focus blocks: Place 2–3 hour focus blocks Tue–Thu (e.g., 9:30–12:00) to guard maker time without impacting any existing commitments.\n",
      "\n",
      "4) Total hours of scheduled meetings\n",
      "- 1.0 hour (two 30-minute meetings)\n",
      "\n",
      "Would you like me to:\n",
      "- Include other calendars (e.g., work, shared, resource calendars) in the analysis?\n",
      "- Re-run using your actual timezone and preferred working hours?\n",
      "\n",
      "🔍 MCP Operations Performed:\n",
      "==============================\n",
      "\n",
      "📋 MCP Call 3:\n",
      "   Tool: search_events\n",
      "   Status: ✅ Success\n",
      "   Output preview: {\"events\": [{\"id\": \"1tnut9gmegv5t0b57talo4i3ng\", \"summary\": \"Chris Alexiuk and Anthony Witherspoon\", \"location\": \"Google Meet (instructions in description)\", \"start\": \"2025-09-08T10:30:00-07:00\", \"end...\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_calendar_operations(token):\n",
    "    \"\"\"\n",
    "    Demonstrate advanced Google Calendar operations including:\n",
    "    - Weekly schedule analysis\n",
    "    - Meeting conflict detection\n",
    "    - Event filtering and search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Advanced calendar analysis query\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\", \n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\",\n",
    "                # Limit to specific tools for focused functionality\n",
    "                \"allowed_tools\": [\"search_events\", \"read_event\"]\n",
    "            }],\n",
    "            input=\"\"\"Analyze my Google Calendar for this week and provide:\n",
    "            1. A summary of my busiest days\n",
    "            2. Any potential scheduling conflicts\n",
    "            3. Recommendations for optimal meeting times\n",
    "            4. Total hours of scheduled meetings\n",
    "            \n",
    "            Please be thorough in your analysis and provide actionable insights.\"\"\"\n",
    "        )\n",
    "        \n",
    "        print(\"📊 Advanced Calendar Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        # Examine the MCP calls that were made\n",
    "        print(\"\\n🔍 MCP Operations Performed:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        for i, output_item in enumerate(response.output):\n",
    "            if output_item.type == \"mcp_call\":\n",
    "                print(f\"\\n📋 MCP Call {i + 1}:\")\n",
    "                print(f\"   Tool: {output_item.name}\")\n",
    "                print(f\"   Status: {'✅ Success' if not output_item.error else '❌ Error'}\")\n",
    "                if output_item.error:\n",
    "                    print(f\"   Error: {output_item.error}\")\n",
    "                else:\n",
    "                    # Show a preview of the output\n",
    "                    output_preview = str(output_item.output)[:200]\n",
    "                    print(f\"   Output preview: {output_preview}...\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with advanced calendar operations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run advanced calendar analysis\n",
    "advanced_calendar_response = demonstrate_advanced_calendar_operations(os.environ[\"GOOGLE_OAUTH_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddc496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GitMCP Server\n",
    "\n",
    "The GitMCP server provides access to documentation and code repositories through a specialized MCP interface. Unlike connectors, this is a remote MCP server that implements the Model Context Protocol for accessing various code documentation resources.\n",
    "\n",
    "**Key Features:**\n",
    "- Documentation search and retrieval\n",
    "- Code analysis and explanation\n",
    "- Technical reference access\n",
    "- Real-time documentation fetching\n",
    "- Specialized tiktoken documentation access\n",
    "\n",
    "**Server Details:**\n",
    "- **Server URL:** `https://gitmcp.io/openai/tiktoken`\n",
    "- **Authentication:** None required for public documentation\n",
    "- **Protocol:** HTTP transport\n",
    "- **Maintained by:** GitMCP (third-party)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e0f65",
   "metadata": {},
   "source": [
    "ℹ️ **No Authentication Required!**\n",
    "\n",
    "The GitMCP server for tiktoken documentation is publicly accessible and doesn't require any authentication tokens. This makes it perfect for demonstrating MCP functionality without setup complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No token required for GitMCP tiktoken documentation server\n",
    "print(\"✅ GitMCP server ready - no authentication required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65d02eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐙 GitHub Analysis Response:\n",
      "==================================================\n",
      "TikToken is OpenAI’s fast byte pair encoding (BPE) tokenizer for mapping text to the integer tokens expected by OpenAI models, and back again.\n",
      "\n",
      "What it does\n",
      "- Converts text to tokens (integers) and back, reversibly and losslessly.\n",
      "- Works on arbitrary Unicode text via a byte-level scheme with “byte fallback,” so every byte sequence can be tokenized.\n",
      "- Uses model-specific vocabularies/merge rules so you get the same tokenization the model was trained with.\n",
      "\n",
      "How it works under the hood\n",
      "1) Special tokens\n",
      "- Certain reserved strings (e.g., <|endoftext|>, chat markers) are mapped to fixed IDs and can be handled explicitly.\n",
      "- You control whether special tokens are allowed in input using parameters like allowed_special or by using encode_ordinary/encode_with_special_tokens.\n",
      "\n",
      "2) Regex pre-tokenization (pat_str)\n",
      "- Text is split into initial chunks using a carefully designed Unicode-aware regex (pat_str) that captures common patterns: spaces, punctuation, word fragments, numbers, etc.\n",
      "- This step approximates linguistic boundaries while remaining purely rule-based.\n",
      "\n",
      "3) Byte-level BPE merges\n",
      "- Each chunk is represented as bytes. A merge table (mergeable_ranks) ranks which adjacent byte sequences to merge.\n",
      "- The algorithm repeatedly merges the highest-priority pair (lowest rank value) until no more merges are allowed, producing subword units that become token IDs.\n",
      "- If a sequence isn’t in the merge table, it falls back to individual byte tokens (ensuring full coverage).\n",
      "\n",
      "4) Decoding\n",
      "- To decode, TikToken maps token IDs back to their byte sequences, concatenates them, and decodes as UTF-8 (plus special tokens when present), reconstructing the original text.\n",
      "\n",
      "Encodings and models\n",
      "- Different OpenAI model families use different base encodings:\n",
      "  - r50k_base / p50k_base (older GPT-2/3 style)\n",
      "  - cl100k_base (GPT-3.5, GPT-4 families, most embeddings)\n",
      "  - o200k_base (newer large-context models like GPT-4o variants)\n",
      "- encoding_for_model(\"model-name\") returns the right encoding for a given model.\n",
      "\n",
      "Performance and implementation\n",
      "- Core implemented in Rust with Python bindings for speed and low memory overhead.\n",
      "- Typically 3–6× faster than comparable open-source tokenizers in benchmarks.\n",
      "\n",
      "Basic API (Python)\n",
      "- Get an encoding:\n",
      "  - import tiktoken\n",
      "  - enc = tiktoken.get_encoding(\"cl100k_base\")\n",
      "  - or enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
      "- Encode/decode:\n",
      "  - ids = enc.encode(\"hello world\")\n",
      "  - text = enc.decode(ids)\n",
      "- Special tokens control:\n",
      "  - enc.encode(\"text <|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
      "  - enc.encode_ordinary(\"...\") ignores special handling\n",
      "  - enc.encode_with_special_tokens(\"...\") always treats specials specially\n",
      "- Count tokens:\n",
      "  - n = len(enc.encode(some_text))\n",
      "\n",
      "Extensibility\n",
      "- You can construct your own Encoding by specifying:\n",
      "  - name, pat_str (regex), mergeable_ranks (vocab/merges), special_tokens (map)\n",
      "- Or register custom encodings via the tiktoken_ext plugin mechanism so get_encoding can find them.\n",
      "\n",
      "Key takeaways\n",
      "- TikToken is a fast, byte-level BPE tokenizer tuned to match OpenAI model training tokenization.\n",
      "- It splits text with a regex, merges bytes via ranked BPE, supports special tokens, and guarantees coverage.\n",
      "- Use encoding_for_model to avoid mismatches and count tokens or prepare inputs reliably.\n"
     ]
    }
   ],
   "source": [
    "# GitMCP Server Example\n",
    "\n",
    "def demonstrate_github_mcp_server():\n",
    "    \"\"\"\n",
    "    Demonstrate GitMCP server functionality.\n",
    "    Shows how to interact with GitHub repositories using the official GitMCP server.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        tools=[{\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"gitmcp\",\n",
    "            \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n",
    "            \"allowed_tools\": [\"search_tiktoken_documentation\", \"fetch_tiktoken_documentation\"],\n",
    "            \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "        }],\n",
    "        input=\"\"\"How does TikToken work?\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"🐙 GitHub Analysis Response:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response.output_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Demonstrate the GitMCP server\n",
    "github_response = demonstrate_github_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae386faf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Summary: OpenAI Responses API with Data and Connectors\n",
    "\n",
    "This notebook demonstrated the full capabilities of the OpenAI Responses API for building AI applications that can access and reason over external data sources. Here's what we covered:\n",
    "\n",
    "### 🔍 File Search Capabilities\n",
    "- **Vector Store Management** - Creating and managing knowledge bases\n",
    "- **Multi-format Support** - PDFs, documents, code files, and more  \n",
    "- **Semantic Search** - Finding information by meaning, not just keywords\n",
    "- **Automatic Citations** - Getting references to source documents\n",
    "- **Retrieval Customization** - Controlling search results and performance\n",
    "\n",
    "### 🔌 MCP Servers and Connectors\n",
    "- **Google Calendar Connector** - Reading and analyzing calendar events with OAuth\n",
    "- **GitMCP** - Checking out the GitHub Docs!\n",
    "\n",
    "### 🛡️ Security and Best Practices\n",
    "- **OAuth Authentication** - Secure token management for external services\n",
    "- **Approval Controls** - Fine-grained permission management\n",
    "- **Error Recovery** - Retry logic and graceful failure handling\n",
    "- **Data Privacy** - Understanding what data is shared with external services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
